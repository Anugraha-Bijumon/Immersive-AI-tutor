# -*- coding: utf-8 -*-
"""MITSUBISHI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvebTaeo8F3NsiB7gwDH6hhV-X6rIW6-
"""

!pip install PyMuPDF faiss-cpu sentence-transformers

import fitz  # PyMuPDF
import re
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

# ----------------------------
# 1. Load PDF
# ----------------------------
pdf_path = "503000542.pdf"
doc = fitz.open(pdf_path)

# Extract all text
full_text = ""
for page in doc:
    text = page.get_text("text")
    full_text += text + "\n"

# Clean up (remove multiple spaces, newlines)
full_text = re.sub(r'\s+', ' ', full_text).strip()

print("Preview of extracted text:\n", full_text[:500])

# ----------------------------
# 2. Smarter Chunking
# ----------------------------

def split_by_lessons(full_text):
    parts = re.split(r'(Lesson\s+\d+[:.])', full_text)
    lessons = []
    for i in range(1, len(parts), 2):
        heading = parts[i].strip()
        body = parts[i+1].strip() if i+1 < len(parts) else ""
        lessons.append(heading + " " + body)
    return lessons

def chunk_lesson(text, chunk_size=250, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

def preprocess_text(full_text):
    lessons = split_by_lessons(full_text)
    if len(lessons) == 0:
      print("âš ï¸ No lesson headings found, falling back to plain chunking...")
      return chunk_lesson(full_text)

    all_chunks = []
    for lesson in lessons:
        lesson_chunks = chunk_lesson(lesson)
        all_chunks.extend(lesson_chunks)
    return all_chunks


# Create chunks
chunks = preprocess_text(full_text)
print(f"Total chunks created: {len(chunks)}")

# ----------------------------
# 3. Embeddings + FAISS (Cosine Similarity)
# ----------------------------
embedder = SentenceTransformer('all-MiniLM-L6-v2')

embeddings = embedder.encode(chunks, convert_to_numpy=True)
embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

print("âœ… FAISS index built with cosine similarity")

# ----------------------------
# 4. Cross-Encoder Reranker
# ----------------------------
from sentence_transformers.cross_encoder import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def retrieve_with_rerank(query, top_k=5):
    # Embed query
    q_emb = embedder.encode([query], convert_to_numpy=True)
    q_emb = q_emb / np.linalg.norm(q_emb)

    # Search in FAISS
    scores, idx = index.search(q_emb.reshape(1, -1), top_k*3)  # get extra for reranking
    candidates = [chunks[i] for i in idx[0]]

    # Rerank
    reranked = rerank_results(query, candidates)
    return reranked[:top_k]

def rerank_results(query, retrieved_chunks):
    pairs = [(query, chunk) for chunk in retrieved_chunks]
    scores = reranker.predict(pairs)
    reranked = [x for _, x in sorted(zip(scores, retrieved_chunks), reverse=True)]
    return reranked

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Load embedding model (lightweight, free)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode chunks
embeddings = model.encode(chunks, convert_to_numpy=True)

# Store in FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"Stored {len(chunks)} embeddings in FAISS index")

# ----------------------------
# 4. Test Retrieval
# ----------------------------
def search(query, k=3):
    q_emb = model.encode([query], convert_to_numpy=True)
    q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)  # normalize
    D, I = index.search(q_emb, k)
    results = []
    for idx, score in zip(I[0], D[0]):
        results.append((chunks[idx], float(score)))
    return results

query = "Why is the Earth called a unique planet?"
results = search(query, k=3)

print("\nQuery:", query)
for i, (text, score) in enumerate(results):
    print(f"\nResult {i+1} (score={score:.4f}):\n{text[:300]}...")

!pip install -U "huggingface_hub[cli]"

from huggingface_hub import login

login()

# ----------------------------
# Phase 4: Interactive Chatbot with RAG + Tracking
# ----------------------------
import csv
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

LOG_FILE = "student_progress.csv"

# Initialize log file with headers (if not exists)
with open(LOG_FILE, "a", newline="") as f:
    writer = csv.writer(f)
    if f.tell() == 0:  # file is empty
        writer.writerow(["timestamp", "student_id", "mode", "query", "result"])

def log_interaction(student_id, mode, query, result):
    with open(LOG_FILE, "a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([datetime.now(), student_id, mode, query, result])

# ----------------------------
# Load Mistral LLM
# ----------------------------
model_name = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",   # auto-places on GPU if available
    torch_dtype="auto"
)

save_path = "mistral_model_local"
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)
print(f"âœ… Model and tokenizer saved at '{save_path}'")

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.3,
    top_p=0.9
)

# ----------------------------
# Core Functions (RAG + Logging)
# ----------------------------
def retrieve_context(query, k=3):
    query_vec = embedder.encode([query], convert_to_numpy=True)
    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)
    D, I = index.search(query_vec, k)
    retrieved_chunks = [chunks[i] for i in I[0]]
    return "\n".join(retrieved_chunks)

def answer_question(query, k=3, student_id="student1"):
    context = retrieve_context(query, k)
    prompt = f"""You are a helpful tutor.
Use the following textbook context to answer the question clearly.

Context:
{context}

Question: {query}
Answer:"""
    response = qa_pipeline(prompt)[0]["generated_text"]
    log_interaction(student_id, "Q&A", query, response)
    return response

def generate_quiz(query, num_q=3, k=3, student_id="student1"):
    context = retrieve_context(query, k)
    prompt = f"""You are a tutor.
From the following context, create {num_q} multiple-choice questions.
Each question should have 4 options (aâ€“d) and mark the correct answer.

Context:
{context}
"""
    response = qa_pipeline(prompt)[0]["generated_text"]
    log_interaction(student_id, "Quiz", query, response)
    return response

def summarize_topic(query, k=3, student_id="student1"):
    context = retrieve_context(query, k)
    prompt = f"""You are a tutor.
Summarize the following context in simple, clear language for a student.

Context:
{context}
"""
    response = qa_pipeline(prompt)[0]["generated_text"]
    log_interaction(student_id, "Summary", query, response)
    return response

# ----------------------------
# Interactive Chatbot Loop
# ----------------------------
print("ðŸ“š Welcome to the AI Tutor! (type 'exit' to quit)\n")
student_id = input("Enter your student ID: ")

while True:
    mode = input("\nChoose mode: [1] Q&A  [2] Quiz  [3] Summary  [exit]: ").strip().lower()
    if mode == "exit":
        print("ðŸ‘‹ Goodbye!")
        break

    query = input("Enter your question/topic: ")

    if mode == "1":
        print("\nAnswer:\n", answer_question(query, student_id=student_id))
    elif mode == "2":
        print("\nQuiz:\n", generate_quiz(query, student_id=student_id))
    elif mode == "3":
        print("\nSummary:\n", summarize_topic(query, student_id=student_id))
    else:
        print("âŒ Invalid choice. Please select 1, 2, 3, or 'exit'.")



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